{"cells":[{"cell_type":"code","source":["!pip install tf2onnx onnxruntime"],"metadata":{"id":"9LVAKNJFJras"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRnT-JAKgCdV"},"source":["# Citations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qrij19CgCdZ"},"outputs":[],"source":["# @misc{tensorflow2015-whitepaper,\n","# title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},\n","# url={https://www.tensorflow.org/},\n","# note={Software available from tensorflow.org},\n","# author={\n","#     Mart\\'{\\i}n~Abadi and\n","#     Ashish~Agarwal and\n","#     Paul~Barham and\n","#     Eugene~Brevdo and\n","#     Zhifeng~Chen and\n","#     Craig~Citro and\n","#     Greg~S.~Corrado and\n","#     Andy~Davis and\n","#     Jeffrey~Dean and\n","#     Matthieu~Devin and\n","#     Sanjay~Ghemawat and\n","#     Ian~Goodfellow and\n","#     Andrew~Harp and\n","#     Geoffrey~Irving and\n","#     Michael~Isard and\n","#     Yangqing Jia and\n","#     Rafal~Jozefowicz and\n","#     Lukasz~Kaiser and\n","#     Manjunath~Kudlur and\n","#     Josh~Levenberg and\n","#     Dandelion~Man\\'{e} and\n","#     Rajat~Monga and\n","#     Sherry~Moore and\n","#     Derek~Murray and\n","#     Chris~Olah and\n","#     Mike~Schuster and\n","#     Jonathon~Shlens and\n","#     Benoit~Steiner and\n","#     Ilya~Sutskever and\n","#     Kunal~Talwar and\n","#     Paul~Tucker and\n","#     Vincent~Vanhoucke and\n","#     Vijay~Vasudevan and\n","#     Fernanda~Vi\\'{e}gas and\n","#     Oriol~Vinyals and\n","#     Pete~Warden and\n","#     Martin~Wattenberg and\n","#     Martin~Wicke and\n","#     Yuan~Yu and\n","#     Xiaoqiang~Zheng},\n","#   year={2015},\n","# }\n","\n","# @misc{chollet2015keras,\n","#   title={Keras},\n","#   author={Chollet, Fran\\c{c}ois and others},\n","#   year={2015},\n","#   howpublished={\\url{https://keras.io}},\n","# }\n","\n","# @article{scikit-learn,\n","#  title={Scikit-learn: Machine Learning in {P}ython},\n","#  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n","#          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n","#          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n","#          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n","#  journal={Journal of Machine Learning Research},\n","#  volume={12},\n","#  pages={2825--2830},\n","#  year={2011}\n","# }\n","\n","# @inproceedings{Yang2016HierarchicalAN,\n","#   title={Hierarchical Attention Networks for Document Classification},\n","#   author={Zichao Yang and Diyi Yang and Chris Dyer and Xiaodong He and Alexander J. Smola and Eduard H. Hovy},\n","#   booktitle={HLT-NAACL},\n","#   year={2016}\n","# }\n","\n","# @Article{Hunter:2007,\n","#   Author    = {Hunter, J. D.},\n","#   Title     = {Matplotlib: A 2D graphics environment},\n","#   Journal   = {Computing In Science \\& Engineering},\n","#   Volume    = {9},\n","#   Number    = {3},\n","#   Pages     = {90--95},\n","#   abstract  = {Matplotlib is a 2D graphics package used for Python\n","#   for application development, interactive scripting, and\n","#   publication-quality image generation across user\n","#   interfaces and operating systems.},\n","#   publisher = {IEEE COMPUTER SOC},\n","#   doi       = {10.1109/MCSE.2007.55},\n","#   year      = 2007\n","# }"]},{"cell_type":"markdown","metadata":{"id":"-OaGCqksgCdp"},"source":["# Importing Packages and Basic Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEDpoknXryB6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('./gdrive', force_remount=True) # 此處需要登入google帳號\n","# 獲取授權碼之後輸入即可連動雲端硬碟"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhXYe16LgCdr"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import os\n","import scipy.io as sio\n","import pickle\n","import matplotlib.pyplot as plt\n","import tf2onnx\n","import onnxruntime as rt\n","import librosa\n","import soundfile as sf\n","# import random as rn\n","\n","from scipy import stats\n","from os import listdir\n","from tensorflow.python.client import device_lib\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Average, Concatenate, GRU, Bidirectional, LeakyReLU, Dense, Dropout, Input, Convolution1D, Layer, Add\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras import regularizers, initializers, constraints\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from tensorflow.keras.losses import categorical_crossentropy, mean_squared_error, binary_crossentropy \n","from tensorflow.keras.utils import plot_model\n","from tqdm import tqdm\n","# import sr_ecg\n","random_seed = 34\n","batch_size = 64\n","num_classes = 1\n","epochs = 500\n","#decide the GPUs to use\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","# #target ECG leads to train\n","# TargetLead = [0,1,2]\n","#Designate the CPSC2018 data set to load\n","data_folder_path = './solicited/'\n","# data_folder_path2 = './TrainingSet2/'\n","# data_folder_path3 = './TrainingSet3/'"]},{"cell_type":"code","source":["def keras_2_onnx(model, modelName):\n","    out_path = modelName + \".onnx\"\n","    spec = (tf.TensorSpec((None, 22050, 1), tf.float32, name=\"input\"),)\n","    spec2 = (tf.TensorSpec((None, 27), tf.float32, name=\"input2\"),)\n","    m_proto, _ = tf2onnx.convert.from_keras(model, input_signature=[spec, spec2], output_path=out_path)\n","    \n","    out_name = [n.name for n in m_proto.graph.output]\n","    return out_name\n","\n","def get_prediction(val_X, modelName):\n","    model_inference = rt.InferenceSession(modelName + \".onnx\")\n","    \n","    input_name = model_inference.get_inputs()[0].name\n","    input_name2 = model_inference.get_inputs()[1].name\n","    label_name = model_inference.get_outputs()[0].name\n","    label_name2 = model_inference.get_outputs()[1].name\n","    onnx_pred = model_inference.run([label_name, label_name2], {input_name: val_X[0].astype(np.float32), input_name2: val_X[1].astype(np.float32)})\n","    # pred = np.argmax(onnx_pred[0], axis=1)\n","    return onnx_pred\n","\n","def get_prediction2(val_X, modelName):\n","    model_inference = rt.InferenceSession(modelName + \".onnx\")\n","    \n","    input_name = model_inference.get_inputs()[0].name\n","    input_name2 = model_inference.get_inputs()[1].name\n","    onnx_pred = model_inference.run(['output'], {input_name: val_X[0].astype(np.float32), input_name2: val_X[1].astype(np.float32)})\n","    # pred = np.argmax(onnx_pred[0], axis=1)\n","    return onnx_pred[0]"],"metadata":{"id":"XLteIDxvKNp5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wM56eLxOgCd7"},"source":["# CPU and GPU Checking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7eoc6megCd7"},"outputs":[],"source":["tf.test.is_gpu_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P20VdIl3rJJE"},"outputs":[],"source":["tf.config.list_physical_devices('GPU')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiSpV7IT95A8"},"outputs":[],"source":["!cp \"./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/solicited.zip\" '.'\n","!unzip solicited.zip -d ./ \n","!rm solicited.zip"]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"lKg5u-oycOXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcQuyGTagCdv"},"outputs":[],"source":["dataList = listdir(data_folder_path)\n","# dataList2 = listdir(data_folder_path2)\n","# dataList3 = listdir(data_folder_path3)\n","\n","# # dataList, dataList2, dataList3\n","len(dataList)"]},{"cell_type":"markdown","metadata":{"id":"AEW05ACTeMij"},"source":["# MetforNetEF Data Combining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw-xbd1nrJJO"},"outputs":[],"source":["# data_folder_path = './gdrive/MyDrive/ColabNotebooks/Competitions/ECGEF/ecg/'\n","TrainTable1 = pd.read_csv('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODA_TB_Clinical_Meta_Info.csv')\n","TrainTable2 = pd.read_csv('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODA_TB_Solicited_Meta_Info.csv')\n","TrainTable = pd.merge(TrainTable2, TrainTable1, on=\"participant\")\n","# dataXList = TrainTable['filename'].values\n","# dataYList = TrainTable['tb_status'].values\n","# TrainPID = TrainTable['participant'].copy()\n","# TrainPID.unique().shape"]},{"cell_type":"code","source":["TrainTable.columns"],"metadata":{"id":"Xf2UAsms-p8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numerical = TrainTable[['filename', 'participant', 'tb_status','sound_prediction_score', 'age', 'height', 'weight', 'reported_cough_dur', 'heart_rate',\n","       'temperature']].copy()\n","categorical = TrainTable[['sex', 'tb_prior', 'tb_prior_Pul',\n","       'tb_prior_Extrapul', 'tb_prior_Unknown', 'hemoptysis', 'weight_loss', 'smoke_lweek', 'fever', 'night_sweats', 'participant']].copy()\n","categorical = pd.get_dummies(categorical).copy()\n","categorical['filename'] = TrainTable['filename']"],"metadata":{"id":"Fw4L2kOI-e7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbUDOlr2rJJP"},"outputs":[],"source":["# TrainTable = pd.concat([numerical,categorical], axis=1, ignore_index=True)\n","TrainTable = pd.merge(numerical, categorical, on=\"filename\")\n","TrainTable.head()"]},{"cell_type":"code","source":["TrainTable.values[:,3:-1082].shape"],"metadata":{"id":"IWsFgDe9DZVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxRConamrJJQ"},"outputs":[],"source":["np.unique(TrainTable.values[:,-1082:].sum(axis = 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbRoO5CXrJJR"},"outputs":[],"source":["AllCol = TrainTable.columns[3:]\n","np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/AllCol',AllCol)\n","AllCol = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/AllCol.npy', allow_pickle=True)\n","AllCol"]},{"cell_type":"code","source":["AllCol[:-1082].shape"],"metadata":{"id":"XgO3mlHT9f07"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataXList = TrainTable['filename'].values.copy()\n","dataYList = TrainTable['tb_status'].values.copy()\n","TrainPID = numerical['participant'].copy()\n","TrainPID.unique().shape"],"metadata":{"id":"cEYv2Anz77FB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wPLDCVSrJJQ"},"outputs":[],"source":["dataXList.shape, dataYList.shape"]},{"cell_type":"code","source":["# import librosa\n","import soundfile as sf\n","maxLength = 0\n","# maxLength2 = 0\n","for i in tqdm(dataXList):\n","    ref, rate = sf.read(data_folder_path+i)\n","    # mel = librosa.feature.melspectrogram(ref, rate, n_fft=1024, hop_length=4, n_mels=32)\n","    # refLength2 = mel.shape[1]\n","    refLength = len(ref)\n","\n","    if maxLength <= refLength:\n","      maxLength = refLength\n","print(maxLength, rate)\n","    # if maxLength2 <= refLength2:\n","    #   maxLength2 = refLength2\n","    #   print(maxLength2, rate)"],"metadata":{"id":"muDVaKWeKBEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# maxLength, maxLength2, mel.shape"],"metadata":{"id":"qqHAFX_xMY10"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMUqN7NprJJU"},"outputs":[],"source":["X_list = []\n","# X_list2 = []\n","Y_list = []\n","for i in tqdm(range(len(dataXList))):\n","    patient = np.zeros((22050,1), dtype=np.float32)\n","    # patient2 = np.zeros((32,5513), dtype=np.float32)\n","    rawValues, rate = sf.read(data_folder_path+dataXList[i])\n","    # print(np.min(rawValues), np.max(rawValues))\n","    mu = np.nanmean(rawValues)\n","    std = np.nanstd(rawValues)\n","    patient[-len(rawValues):,0] = (rawValues-mu)/std\n","    # rawValues2 = librosa.feature.melspectrogram(rawValues, rate, n_fft=1024, hop_length=4, n_mels=32)\n","    # patient2[:,-rawValues2.shape[1]:] = rawValues2\n","    # patient[-len(rawValues):,0] = (rawValues)/std\n","\n","    X_list.append(patient)\n","    # X_list2.append(patient2.T)\n","    Y_list.append(dataYList[i])\n","X_list = np.asarray(X_list)\n","# X_list2 = np.asarray(X_list2)\n","Y_list = np.asarray(Y_list)\n","print(X_list.shape, Y_list.shape)\n","# print(X_list.shape, X_list2.shape, Y_list.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWY4EO5IrJJV"},"outputs":[],"source":["np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAX_list',X_list)\n","# np.save('./CODAX_list2',X_list2)\n","np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAY_list',Y_list)"]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"A6diT2pZoSNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kn2GKQUXrJJV"},"outputs":[],"source":["X_list_label = TrainTable.copy()\n","X_list_label.values[:,3:-1082].shape"]},{"cell_type":"code","source":["mus = []\n","stds = []\n","for i in range(7):\n","  mu = np.nanmean(X_list_label[AllCol[i]].astype('float32'))\n","  std = np.nanstd(X_list_label[AllCol[i]].astype('float32'))\n","  X_list_label[AllCol[i]] = (X_list_label[AllCol[i]].astype('float32')-mu)/std\n","  X_list_label[AllCol[i]] = X_list_label[AllCol[i]].fillna(0).copy()\n","  mus.append(mu)\n","  stds.append(std)\n","CODAMuStd = np.asarray([mus,stds])\n","CODAMuStd"],"metadata":{"id":"DjjH0YHs-yHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAMuStd',CODAMuStd)"],"metadata":{"id":"-Xdn0NwA-2DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mus = []\n","stds = []\n","for i in range(6):\n","  mu = np.nanmean(X_list_label[AllCol[i]].astype('float32'))\n","  std = np.nanstd(X_list_label[AllCol[i]].astype('float32'))\n","  X_list_label[AllCol[i]] = (X_list_label[AllCol[i]].astype('float32')-mu)/std\n","  X_list_label[AllCol[i]] = X_list_label[AllCol[i]].fillna(0).copy()\n","  mus.append(mu)\n","  stds.append(std)\n","CODAMuStd = np.asarray([mus,stds])\n","np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAMuStd',CODAMuStd)\n","CODAMuStd"],"metadata":{"id":"DOKgkrd9igzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CODAMuStd = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAMuStd.npy', allow_pickle=True).astype('float32')\n","CODAMuStd"],"metadata":{"id":"hwY_fgy0Thk5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-5zE6qprJJW"},"outputs":[],"source":["np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAX_list_label',X_list_label.values[:,3:])"]},{"cell_type":"code","source":["X_list_label.values[:,3:-1082].shape"],"metadata":{"id":"9e_i_JPG9_tv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYkTH1t0e-Mo"},"source":["# MetforNetEF Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2iH3c8dQwLQ"},"outputs":[],"source":["X_list = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAX_list.npy',allow_pickle=True).astype('float32')\n","# X_list = np.load('./CODAX_list2.npy',allow_pickle=True).astype('float32')\n","Y_list = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAY_list.npy',allow_pickle=True).astype('float32')\n","X_list_label = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/CODAX_list_label.npy',allow_pickle=True).astype('float32')[:,:-1082]\n","# X_list_test = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/ECGEFX_list_test.npy',allow_pickle=True)\n","# X_list_label_test = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/ECGEFX_list_label_test.npy',allow_pickle=True)"]},{"cell_type":"code","source":["X_list_label.shape"],"metadata":{"id":"aRJVKEpYsVrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFXXRHJHrJJf"},"outputs":[],"source":["data = Y_list\n","count, bins_count = np.histogram(data, bins=1000)\n","pdf = count / sum(count)\n","cdf = np.cumsum(pdf)\n","plt.plot(bins_count[1:], pdf, label=\"PDF\")\n","plt.legend()\n","# plt.ylim([-0.01, 0.08])\n","# plt.xlim([-0.01, 1.0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8dU1kfprJJp"},"outputs":[],"source":["plt.plot(X_list[0,:,0])\n","# plt.ylim([-6.0, 1.0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0Y6X_uCIfcFw"},"source":["# MetforNetEF Splitting Train-Validation Data"]},{"cell_type":"code","source":["X = X_list\n","y = Y_list\n","skf = StratifiedKFold(n_splits=10, random_state = random_seed, shuffle=True)\n","skf.get_n_splits(X, y)\n","print(skf)  \n","FoldIndicesList = []\n","for train_index, test_index in skf.split(X, y):\n","    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n","    FoldIndicesList.append((train_index,test_index))\n","FoldIndicesList = np.asarray(FoldIndicesList)\n","np.save('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/FoldIndicesList',FoldIndicesList)\n","FoldIndicesList = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/FoldIndicesList.npy',allow_pickle=True)"],"metadata":{"id":"D4jh8-k2iSIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FoldIndicesList.shape"],"metadata":{"id":"RtX2w6ZinqqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1srDO3qlgCd9"},"source":["# MetforNetEF Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8aUapzrgCd-"},"outputs":[],"source":["def dot_product(x, kernel):\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","    \n","class AttentionWithContext(Layer):\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs): \n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform') \n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer) \n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint) \n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n"," \n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight(shape=(input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint) \n","            self.u = self.add_weight(shape=(input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint) \n","        super(AttentionWithContext, self).build(input_shape)\n"," \n","    def compute_mask(self, input, input_mask=None):\n","        return None\n"," \n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W) \n","        if self.bias:\n","            uit += self.b \n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u) \n","        a = K.exp(ait)\n","        if mask is not None:\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) \n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n"," \n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLs7Dv5OrJJl"},"outputs":[],"source":["#model structure\n","def MetforNet121(length):\n","  \n","    main_input = Input(shape=(length,1), dtype='float32', name='main_input')\n","    x = Convolution1D(12, 3, padding='same')(main_input)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 3, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Convolution1D(12, 48, strides = 2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    cnnout = Dropout(0.2)(x)\n","    x = Bidirectional(GRU(12,return_sequences=True,return_state=False))(cnnout)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = AttentionWithContext()(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    main_output = Dense(1,activation='sigmoid')(x)\n","\n","    return Model(inputs=main_input, outputs=main_output)\n","model = MetforNet121(5513)\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXvyYm5YPgpc"},"outputs":[],"source":["#model structure\n","def MetforNetEF(length, model):\n","  \n","    main_input2 = Input(shape=(27,), dtype='float32', name='main_input2')\n","    x = model.layers[-2].output\n","    x = Concatenate(axis = -1)([x, main_input2])\n","    x = Dense(24,activation=None)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.3)(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(1,activation='tanh')(x)\n","    main_output = Add()([x, model.layers[-1].output])\n","\n","    return Model(inputs=[model.input, main_input2], outputs=[main_output, model.layers[-1].output])\n","model = MetforNetEF(5513,MetforNet121(5513))\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"RZNFhLtbhkYE"},"source":["# MetforNetEF Model Training and 10-fold Cross Validation\n","\n"]},{"cell_type":"code","source":["#10-fold training\n","FoldIndicesList = np.load('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/FoldIndicesList.npy',allow_pickle=True)\n","lr = 0.001\n","epochs = 100\n","for i in range(10):\n","    model_name = 'CODA_PID_10dense_50folds_sigmoid_fold_double_separate_'+str(i)\n","    x_train_from_train = X_list[np.setdiff1d(FoldIndicesList[i][0],FoldIndicesList[i-1][1])]\n","    x_val_from_train = X_list[FoldIndicesList[i-1][1]]\n","    x_train_from_train_label = X_list_label[np.setdiff1d(FoldIndicesList[i][0],FoldIndicesList[i-1][1])]\n","    x_val_from_train_label = X_list_label[FoldIndicesList[i-1][1]]\n","    y_train_from_train = Y_list[np.setdiff1d(FoldIndicesList[i][0],FoldIndicesList[i-1][1])]\n","    y_val_from_train = Y_list[FoldIndicesList[i-1][1]]\n","    pre_model = MetforNet121(22050)\n","    model = MetforNetEF(22050,pre_model)\n","\n","    opt = keras.optimizers.Adam(lr=lr)\n","    model.compile(loss=mean_squared_error,\n","                optimizer=opt,\n","                metrics=['accuracy'])\n","    checkpointer = ModelCheckpoint('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/'+model_name, verbose=1, save_best_only=True)\n","    model_history = model.fit( [x_train_from_train, x_train_from_train_label], [y_train_from_train,y_train_from_train],\n","                            batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpointer],\n","                            validation_data=([x_val_from_train, x_val_from_train_label], [y_val_from_train,y_val_from_train]))\n","    with open('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/'+model_name+'_history.pickle', 'wb') as file:\n","        pickle.dump(model.history.history, file)\n"],"metadata":{"id":"7r7sWXjMlw9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUCScores = []\n","AUCScores2 = []\n","for i in tqdm(range(10)):\n","  pre_model = MetforNet121(22050)\n","  model = MetforNetEF(22050, pre_model)\n","  model.load_weights('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/CODA_PID_10dense_50folds_sigmoid_fold_double_separate_'+str(i))\n","  out_name = keras_2_onnx(model, './gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/CODA_PID_10dense_50folds_sigmoid_fold_double_separate_'+str(i))\n","  predicted_val, predicted_val2 = get_prediction([X_list[FoldIndicesList[i][1]], X_list_label[FoldIndicesList[i][1]][:,1:]], './gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/CODA_PID_10dense_50folds_sigmoid_fold_double_separate_'+str(i))\n","  AUCScores.append(roc_auc_score((Y_list[FoldIndicesList[i][1]]>0.5), predicted_val[:,0]))\n","  AUCScores2.append(roc_auc_score((Y_list[FoldIndicesList[i][1]]>0.5), predicted_val2[:,0]))\n"],"metadata":{"id":"XUK4D9S5LmFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_val.shape, predicted_val2.shape"],"metadata":{"id":"xbjkqXETCqef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('AUCScores 1')\n","print(AUCScores)\n","print(\"Median AUC: \", np.median(AUCScores))\n","print(\"Mean AUC: \", np.mean(AUCScores))"],"metadata":{"id":"CcT6IMagnyzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('AUCScores 2')\n","print(AUCScores2)\n","print(\"Median AUC: \", np.median(AUCScores2))\n","print(\"Mean AUC: \", np.mean(AUCScores2))"],"metadata":{"id":"SbmY57WR-O6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main_input = Input(shape=(22050,1), dtype='float32', name='main_input')  \n","main_input2 = Input(shape=(27,), dtype='float32', name='main_input2')\n","outputs = []\n","for i in tqdm(range(10)):\n","  pre_model = MetforNet121(22050)\n","  model = MetforNetEF(22050, pre_model)\n","  model.load_weights('./gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/CODA_PID_10dense_50folds_sigmoid_fold_double_separate_'+str(i))\n","  model.layers.pop(0)\n","  newOutputs = model([main_input, main_input2])\n","  outputs.append(newOutputs)\n","main_output = Average(name='output')([i[0] for i in outputs])\n","main_output2 = Average(name='output2')([i[1] for i in outputs])\n","fin_model = Model(inputs=[main_input, main_input2], outputs=[main_output, main_output2])\n","print(fin_model.summary())"],"metadata":{"id":"vIcnDKCoF8Gi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out_name = keras_2_onnx(fin_model, './gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/model')\n","predicted_val = np.zeros(Y_list.shape)\n","for i in tqdm(range(predicted_val.shape[0])):\n","  predicted_temp = get_prediction2([X_list[i:i+1,:], X_list_label[i:i+1,:]], './gdrive/MyDrive/ColabNotebooks/Competitions/CODA/double_separate/model')\n","  predicted_val[i] += predicted_temp[0,0]"],"metadata":{"id":"r1wMGkFpGZds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roc_auc_score((Y_list>0.5), predicted_val)"],"metadata":{"id":"Yo_w-54yGgdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_val2 = predicted_val.copy()\n","predicted_val2[predicted_val2<0] = 0\n","predicted_val2[predicted_val2>=1] = 1"],"metadata":{"id":"XQR3CUOIxGAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["roc_auc_score((Y_list>0.5), predicted_val2)"],"metadata":{"id":"2ekyW2tHxEs5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference code"],"metadata":{"id":"A_INXh_ligrq"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","import onnxruntime as rt\n","import pandas as pd\n","import numpy as np\n","import soundfile as sf\n","\n","\n","def prediction(input_path, model_path):\t\n","    switch = 1\n","    AllCol = ['participant', 'filename', 'age', 'height', 'weight',\n","          'reported_cough_dur', 'heart_rate', 'temperature', 'sex_Female',\n","          'sex_Male', 'tb_prior_No', 'tb_prior_Not sure', 'tb_prior_Yes',\n","          'tb_prior_Pul_No', 'tb_prior_Pul_Yes', 'tb_prior_Extrapul_No',\n","          'tb_prior_Extrapul_Yes', 'tb_prior_Unknown_No',\n","          'tb_prior_Unknown_Yes', 'hemoptysis_No', 'hemoptysis_Yes',\n","          'weight_loss_No', 'weight_loss_Yes', 'smoke_lweek_No',\n","          'smoke_lweek_Yes', 'fever_No', 'fever_Yes', 'night_sweats_No',\n","          'night_sweats_Yes']\n","    CODAMuStd = np.asarray([[ 40.817745 , 161.45787  ,  57.327652 ,  46.227486 ,  87.421104 ,\n","            36.733143 ],\n","          [ 15.1564245,   8.7601595,  13.59417  ,  51.92165  ,  16.980263 ,\n","              0.5627685]])\n","    input_df = pd.read_csv(input_path+'meta_info.csv')[['participant', 'filename']]\n","    X_list_label = np.zeros((input_df.shape[0], 27))\n","\n","    try:\n","        input_df2 = pd.read_csv(input_path+'CODA_TB_Clinical_Meta_Info_Test.csv')\n","        input_df3 = pd.merge(input_df2, input_df, on=\"participant\")\n","        numerical = input_df3[['filename', 'participant', 'age', 'height', 'weight', 'reported_cough_dur', 'heart_rate',\n","            'temperature']].copy()\n","        categorical = input_df3[['sex', 'tb_prior', 'tb_prior_Pul',\n","            'tb_prior_Extrapul', 'tb_prior_Unknown', 'hemoptysis', 'weight_loss', 'smoke_lweek', 'fever', 'night_sweats', 'participant']].copy()\n","        categorical = pd.get_dummies(categorical).copy()\n","        categorical['filename'] = input_df3['filename']\n","        input_df3 = pd.merge(numerical, categorical, on=\"filename\")\n","        temp = input_df3[['participant', 'filename']].copy()\n","        input_df = temp\n","        \n","        switch = 0        \n","        for i in range(6):\n","            try:\n","                input_df3[AllCol[2:][i]] = (input_df3[AllCol[2:][i]].astype('float32')-CODAMuStd[0, i])/CODAMuStd[1, i]\n","                input_df3[AllCol[2:][i]] = input_df3[AllCol[2:][i]].fillna(0).copy()\n","                temp = input_df3[AllCol[2:][i]].values\n","                X_list_label[:, i] += temp\n","            except:\n","                X_list_label[:, i] = np.zeros(X_list_label.shape[0])\n","                \n","        for i in range(21):\n","            try:\n","                temp = input_df3[AllCol[2:][i+6]].values\n","                X_list_label[:, i+6] += temp\n","            except:\n","                X_list_label[:, i+6] = np.zeros(X_list_label.shape[0])\n","    except:\n","        input_df = pd.read_csv(input_path+'meta_info.csv')[['participant', 'filename']]\n","\n","    dataXList = input_df['filename'].values\n","    X_list = []\n","    for i in range(len(dataXList)):\n","        patient = np.zeros((22050,1), dtype=np.float32)\n","        rawValues, rate = sf.read(input_path+'raw_test_data/'+dataXList[i])\n","        mu = np.nanmean(rawValues)\n","        std = np.nanstd(rawValues)\n","        patient[-len(rawValues):, 0] = (rawValues-mu)/std\n","        X_list.append(patient)\n","    X_list = np.asarray(X_list)\n","\n","    pred = np.zeros(X_list.shape[0])\n","    model_inference = rt.InferenceSession(model_path+'model.onnx')\n","    input_name = model_inference.get_inputs()[0].name\n","    input_name2 = model_inference.get_inputs()[1].name\n","    label_name = model_inference.get_outputs()[0].name\n","    label_name2 = model_inference.get_outputs()[1].name    \n","    for i in range(X_list.shape[0]):\n","        onnx_pred = model_inference.run([label_name, label_name2], {input_name: X_list[i:i+1, :].astype(np.float32), input_name2: X_list_label[i:i+1, :].astype(np.float32)})\n","        pred[i] += onnx_pred[switch][0, 0]\n","\n","    #Paitent id\n","    df_pred = pd.DataFrame(input_df['participant'], columns=['participant']) \n","    df_pred['probability'] = pred\n","    df_pred_temp = df_pred.dropna().groupby('participant')['probability'].max()\n","    df_pred_temp_values = df_pred_temp.values\n","    df_pred_temp_values[df_pred_temp_values>=1] = 1\n","    df_pred_temp_values[df_pred_temp_values<0] = 0\n","    df_pred_fin = pd.DataFrame(np.asarray([df_pred_temp.index, df_pred_temp_values]).T, columns=['participant','probability'])\n","    df_pred_fin.to_csv('./output/predictions.csv', index=False)\n","\n","    # comfirm that probability of predictions.csv exists\n","    print(pd.read_csv('./output/predictions.csv')['probability'])\n","    print('mode :', switch)\n","\n","\n","if __name__ == '__main__':    \n","    #holdout sample file path \n","    input_path = './input/' # Utilize {'meta_info.csv'  'raw_test_data/*.wav'} according to your preprocessing logic\n","    # In SC2 you may also load CODA_TB_Clinical_Meta_Info_Test.csv\n","    # however attempting to load it in SC1 will result in failure\n","    \n","    #model path\n","    model_path = './model/'\n","    prediction(input_path, model_path)\n"],"metadata":{"id":"52u-Iq3PifT-"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}